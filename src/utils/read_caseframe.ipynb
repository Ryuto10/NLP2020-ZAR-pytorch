{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_predict(vocab):\n",
    "    gold = None\n",
    "    with open(predict_file) as fp, open(test_file) as ft:\n",
    "        for line in fp:\n",
    "            predict = json.loads(line)\n",
    "            if gold is None or gold[\"file name\"] != predict[\"file\"] or gold[\"sentence id\"] != predict[\"sent\"]:                \n",
    "                gold = json.loads(next(ft))\n",
    "                \n",
    "            gold_args = None\n",
    "            for pas in gold[\"pas\"]:\n",
    "                if pas[\"p_id\"] == predict[\"pred\"]:\n",
    "                    gold_args = pas[\"args\"]\n",
    "                    break\n",
    "            if gold_args is None:\n",
    "                raise RuntimeError\n",
    "                \n",
    "            tokens = [vocab[idx] for idx in gold[\"tokens\"]]\n",
    "            predicate = tokens[predict[\"pred\"]]\n",
    "            \n",
    "            for idx, case in enumerate([\"ga\", \"o\", \"ni\"]):\n",
    "                prd_term = tokens[predict[case]] if case in predict else \"None\"\n",
    "                gold_term = tokens[gold_args.index(idx)] if idx in gold_args else \"None\"\n",
    "                if prd_term == \"None\" and gold_term == \"None\":\n",
    "                    continue\n",
    "                if case in predict and gold_args[predict[case]] == idx:\n",
    "                    yield (\"correct\", case, predicate, gold_term, tokens)\n",
    "                else:\n",
    "                    yield (prd_term, case, predicate, gold_term, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_combination(before, after):\n",
    "    if len(after) == 1:\n",
    "        for word in after[0]:\n",
    "            yield before + word\n",
    "    else:\n",
    "        for word in after[0]:\n",
    "            for text in chain_combination(before + word, after[1:]):\n",
    "                yield text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_rep(rep):\n",
    "    chain = [[token.split(\"/\")[0] for token in chunk.split(\"?\")] for chunk in rep.split(\"+\")]\n",
    "    for text in chain_combination(\"\", chain):\n",
    "        yield text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(root):\n",
    "    frames = {}\n",
    "    for entry in tqdm(root):\n",
    "        for predicate in concat_rep(entry.attrib[\"headword\"]):\n",
    "            extract_cases = defaultdict(lambda : {})\n",
    "            for caseframe in entry:\n",
    "                for case in caseframe:\n",
    "                    for cn, cl in [(\"ガ格\", \"ga\"), (\"ヲ格\", \"o\"), (\"ニ格\", \"ni\")]:\n",
    "                        if case.attrib[\"case\"] == cn:\n",
    "                            for comp in case:\n",
    "                                for word in concat_rep(comp.text):\n",
    "                                    extract_cases[cl][word] = comp.attrib['frequency']\n",
    "            frames[predicate] = extract_cases\n",
    "    \n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "caseframe_file = \"/Users/ryuto/lab/research/data/raw/kyoto-univ-web-cf-1.0/kyoto-univ-web-cf-1.0.xml\"\n",
    "\n",
    "tree = ET.parse(caseframe_file)\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34059/34059 [00:30<00:00, 1125.85it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "frames = extract_frames(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = \"/Users/ryuto/lab/research/data/raw/kyoto-univ-web-cf-1.0/case-frame.json\"\n",
    "\n",
    "with open(out_file, \"w\") as fo:\n",
    "    json.dump(frames, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_file = \"/Users/ryuto/lab/research/work/ACL2020/predict-dev-base_full-olr0.001_plr0.001_h256_layer10_d0.0_True_size100-0.4-0.52-0.13.txt\"\n",
    "test_file = \"/Users/ryuto/lab/research/work/ACL2020/dev.jsonl\"\n",
    "wordindex_file = \"/Users/ryuto/lab/research/data/raw/NTC_Matsu_original/wordIndex.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vocab():\n",
    "    vocab = {}\n",
    "    with open(wordindex_file) as fi:\n",
    "        for line in fi:\n",
    "            word, index = line.rstrip(\"\\n\").split(\"\\t\", 1)\n",
    "            vocab[index] = word\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = read_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = []\n",
    "recall = []\n",
    "co = []\n",
    "correct = []\n",
    "\n",
    "for p, case, prd, g, tokens in extract_predict(vocab):\n",
    "    if p == \"None\":\n",
    "        recall.append((p, case, prd, g))\n",
    "    elif g == \"None\":\n",
    "        prec.append((p, case, prd, g))\n",
    "    elif p == \"correct\":\n",
    "        correct.append((p, case, prd, g))\n",
    "    else:\n",
    "        co.append((p, case, prd, g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
