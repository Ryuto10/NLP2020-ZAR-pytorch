{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/Users/ryuto/lab/research/work/ACL2020/train.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from itertools import islice\n",
    "\n",
    "def read_file(file):\n",
    "    with open(file) as fi:\n",
    "        for line in fi:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            yield json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_vocab(vocab_file):\n",
    "    \"\"\"Open file of pre-trained vocab and convert to dict format.\"\"\"\n",
    "    print(\"\\n# Load '{}'\".format(vocab_file))\n",
    "    vocab = {}\n",
    "    f = open(vocab_file)\n",
    "    for line in f:\n",
    "        split_line = line.rstrip().split(\"\\t\")\n",
    "        word, idx = split_line[0], split_line[1]\n",
    "        vocab[word] = idx\n",
    "    f.close()\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import sliding_window\n",
    "import random\n",
    "\n",
    "MASK = \"M\"\n",
    "\n",
    "def create_before_before(instance, n_insert=(3, 5)):\n",
    "    case_converter = {0: \"が\", 1: \"を\", 2: \"に\"}\n",
    "    phrase_ids = [idx for idx, v in enumerate(instance[\"bunsetsu\"]) if v == 1] + [len(instance[\"tokens\"])]\n",
    "    phrase_range = [(sta, end) for sta, end in sliding_window(2, phrase_ids)]    \n",
    "    tree = [p for p in instance[\"tree\"] if p]\n",
    "    assert len(phrase_range) == len(tree)\n",
    "    \n",
    "    print(\" \".join(\"\".join(instance[\"surfaces\"][sta:end]) for sta, end in phrase_range))\n",
    "    \n",
    "    for pas in instance[\"pas\"]:\n",
    "        predicate = instance[\"surfaces\"][pas[\"p_id\"]]\n",
    "        zero_ids = [idx for idx, t in enumerate(pas[\"types\"]) if t == \"zero\"]\n",
    "        \n",
    "        for zero_idx in zero_ids:\n",
    "            args = [instance[\"surfaces\"][zero_idx], case_converter[pas[\"args\"][zero_idx]], predicate]\n",
    "            \n",
    "            # Create Path from zero\n",
    "            path_zero = {}\n",
    "            target = None\n",
    "            for (sta, end), (index, head) in zip(phrase_range, tree):\n",
    "                if target is None and sta <= zero_idx < end:\n",
    "                    path_zero[index] = list(range(sta, end))\n",
    "                    args_idx = index\n",
    "                    target = head\n",
    "                if target is not None and target == index:\n",
    "                    path_zero[index] = list(range(sta, end))\n",
    "                    target = head\n",
    "\n",
    "            target = None\n",
    "            for (sta, end), (index, head) in zip(phrase_range, tree):\n",
    "                if target is None and sta <= zero_idx < end:\n",
    "                    path_zero[index] = list(range(sta, end))\n",
    "                    target = index\n",
    "                if target is not None and target == index:\n",
    "                    path_zero[index] = list(range(sta, end))\n",
    "                    target = index\n",
    "                    \n",
    "            # Create Path from predicate\n",
    "            path_predicate_after = {}\n",
    "            predicate_idx = None\n",
    "            target = None\n",
    "            for (sta, end), (index, head) in zip(phrase_range, tree):\n",
    "                if target is None and sta <= pas[\"p_id\"] < end:\n",
    "                    path_predicate_after[index] = list(range(sta, end))\n",
    "                    predicate_idx = index\n",
    "                    target = head\n",
    "                if target is not None and target == index:\n",
    "                    path_predicate_after[index] = list(range(sta, end))\n",
    "                    target = head\n",
    "\n",
    "            path_predicate_before = {}\n",
    "            target = None\n",
    "            for (sta, end), (index, head) in zip(phrase_range[::-1], tree[::-1]):\n",
    "                if target is None and sta <= pas[\"p_id\"] < end:\n",
    "                    path_predicate_before[index] = list(range(sta, end))\n",
    "                    target = index\n",
    "                if target is not None and target == head:\n",
    "                    path_predicate_before[index] = list(range(sta, end))\n",
    "                    target = index\n",
    "\n",
    "            if predicate_idx < args_idx:\n",
    "                continue\n",
    "                    \n",
    "            # Merge path & Create text_a\n",
    "            end_idx = min(path_zero.keys() & path_predicate_after.keys())\n",
    "            fake_args_idx = min(path_predicate_before)\n",
    "            path_zero.update(path_predicate_after)\n",
    "            path_zero.update(path_predicate_before)\n",
    "            merged_indices, insert_position, n_words, before_idx = [], [], 0, None\n",
    "            text_a = [MASK] * random.randint(*n_insert)\n",
    "            for k in sorted(path_zero):\n",
    "                if before_idx is None:\n",
    "                    before_idx = k\n",
    "                elif k == fake_args_idx:\n",
    "                    insert_position.append(n_words)\n",
    "                    text_a += [MASK] * random.randint(*n_insert)\n",
    "                    text_a += [\"、、\", instance[\"surfaces\"][zero_idx], case_converter[pas[\"args\"][zero_idx]]]\n",
    "                elif before_idx + 1 != k:\n",
    "                    insert_position.append(n_words)\n",
    "                    text_a += [MASK] * random.randint(*n_insert)\n",
    "                merged_indices += path_zero[k]\n",
    "                text_a += [instance[\"surfaces\"][idx] for idx in path_zero[k]]\n",
    "                n_words += len(path_zero[k])\n",
    "                if k == end_idx:\n",
    "                    break\n",
    "                before_idx = k\n",
    "            if end_idx != tree[-1][0]:\n",
    "                text_a += [MASK] * random.randint(*n_insert)\n",
    "                insert_position.append(n_words)\n",
    "            \n",
    "            mask_ids = [idx for idx, surface in enumerate(text_a) if surface == MASK]\n",
    "                    \n",
    "            # Create new instance\n",
    "            keys = [\"tokens\", \"surfaces\", \"bases\", \"pos\", \"bunsetsu\", \"tree\"]\n",
    "            new_instance = {k: [instance[k][idx] for idx in merged_indices] for k in keys}\n",
    "            new_instance[\"insert_position\"] = insert_position\n",
    "            new_instance[\"text_a\"] = text_a\n",
    "            new_instance[\"mask_ids\"] = mask_ids\n",
    "            \n",
    "            yield args, new_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "村山富市首相は 年頭に あたり 首相官邸で 内閣記者会と 二十八日 会見し、 社会党の 新民主連合所属議員の 離党問題に ついて 「政権に 影響を 及ぼす ことには ならない。 離党者が いても、 その 範囲に とどまると 思う」と 述べ、 大量離党には 至らないとの 見通しを 示した。\n",
      "問題, が, 及ぼす \n",
      "\t離党問題について影響を及ぼすことにはならない。思う」と述べ、 \n",
      "\tMMMMM離党問題についてMMMMM、、問題が影響を及ぼすことにはならない。MMMM思う」と述べ、MMM。\n",
      "問題, が, なら \n",
      "\t離党問題について影響を及ぼすことにはならない。思う」と述べ、 \n",
      "\tMMM離党問題についてMMMM、、問題が影響を及ぼすことにはならない。MMM思う」と述べ、MMMMM。\n",
      "問題, が, とどまる \n",
      "\t離党問題についてその範囲にとどまると思う」と述べ、 \n",
      "\tMMMM離党問題についてMMMMM、、問題がその範囲にとどまると思う」と述べ、MMMM。\n",
      "首相, が, 思う \n",
      "\t村山富市首相は会見し、その範囲にとどまると思う」と述べ、示した。 \n",
      "\tMMM村山富市首相はMMMM会見し、MMM、、首相がその範囲にとどまると思う」と述べ、MMMMM示した。\n",
      "首相, が, 述べ \n",
      "\t村山富市首相は会見し、その範囲にとどまると思う」と述べ、示した。 \n",
      "\tMMMMM村山富市首相はMMMM会見し、MMM、、首相がその範囲にとどまると思う」と述べ、MMMM示した。\n",
      "問題, が, 至ら \n",
      "\t離党問題について述べ、大量離党には至らないとの見通しを示した。 \n",
      "\tMMM離党問題についてMMMM述べ、MMMM、、問題が大量離党には至らないとの見通しを示した。\n",
      "首相, が, 示した \n",
      "\t村山富市首相は会見し、大量離党には至らないとの見通しを示した。 \n",
      "\tMMMMM村山富市首相はMMM会見し、MMMM、、首相が大量離党には至らないとの見通しを示した。\n",
      "また、 一九九五年中の 衆院解散・総選挙の 可能性に 否定的な 見解を 表明、 二十日 召集予定の 通常国会前の 内閣改造を 明確に 否定した。\n",
      "ロシア南部チェチェン共和国の 首都グロズヌイに 進攻した ロシア軍は 三十一日、 首都中心部を 装甲車などで 攻撃、 大統領官邸など 数カ所が 炎上した。\n",
      "ロシア側は 首都制圧の 最終 段階に 入ったと みられる。\n",
      "制圧, が, 入った \n",
      "\t首都制圧の最終段階に入ったと \n",
      "\tMMM首都制圧のMMMM、、制圧が最終段階に入ったとMMMM。\n",
      "グロズヌイからの 報道では、 ロシア軍は 激しい 空爆と 砲撃を 加えた 後、 装甲車部隊が 大統領官邸付近に 進出。\n",
      "軍, が, 加えた \n",
      "\tロシア軍は空爆と砲撃を加えた後、進出。 \n",
      "\tMMMMロシア軍はMMMMM、、軍が空爆と砲撃を加えた後、MMMM進出。\n",
      "同官邸前などで ドゥダエフ政権部隊と 激しい 市街戦を 展開している。\n",
      "一方、 ドゥダエフ政権側の 首都防衛司令官は 同日 夕、 テレビを 通じ、 首都防衛は うまく いっており、 ロシア軍の 戦車 五十両を 破壊したと 発表。\n",
      "官, が, 通じ \n",
      "\t首都防衛司令官はテレビを通じ、いっており、発表。 \n",
      "\tMMMM首都防衛司令官はMMM、、官がテレビを通じ、MMMMいっており、MMMM発表。\n",
      "また、 ドゥダエフ大統領は 現在、 交渉中の ロシア議会の 代表団とともに 防空ごうに 避難しており、 無事と いう。\n",
      "大統領, が, 無事 \n",
      "\tドゥダエフ大統領は防空ごうに避難しており、無事と \n",
      "\tMMMドゥダエフ大統領はMMMMM、、大統領が防空ごうに避難しており、無事とMMM。\n",
      "ドゥダエフ大統領は 三十日 夜、 エリツィン・ロシア大統領に 正月休戦を 提案したが、 ロシア側は これを 黙殺した。\n",
      "村山富市首相の 年頭記者会見の 要旨は 次の 通り。\n"
     ]
    }
   ],
   "source": [
    "for instance in islice(read_file(file), 10):\n",
    "    for args, instance in create_before_before(instance):\n",
    "        print(\", \".join(args), \n",
    "              \"\\n\\t\"+\"\".join(instance[\"surfaces\"]), \n",
    "              \"\\n\\t\"+\"\".join(instance[\"text_a\"]).rstrip(\"。\")+\"。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import sliding_window\n",
    "import random\n",
    "\n",
    "MASK = \"M\"\n",
    "\n",
    "def create_before_after(instance, n_insert=(3, 5)):\n",
    "    case_converter = {0: \"が\", 1: \"を\", 2: \"に\"}\n",
    "    phrase_ids = [idx for idx, v in enumerate(instance[\"bunsetsu\"]) if v == 1] + [len(instance[\"tokens\"])]\n",
    "    phrase_range = [(sta, end) for sta, end in sliding_window(2, phrase_ids)]    \n",
    "    tree = [p for p in instance[\"tree\"] if p]\n",
    "    assert len(phrase_range) == len(tree)\n",
    "    \n",
    "    print(\" \".join(\"\".join(instance[\"surfaces\"][sta:end]) for sta, end in phrase_range))\n",
    "    \n",
    "    for pas in instance[\"pas\"]:\n",
    "        predicate = instance[\"surfaces\"][pas[\"p_id\"]]\n",
    "        zero_ids = [idx for idx, t in enumerate(pas[\"types\"]) if t == \"zero\"]\n",
    "        \n",
    "        for zero_idx in zero_ids:\n",
    "            args = [instance[\"surfaces\"][zero_idx], case_converter[pas[\"args\"][zero_idx]], predicate]\n",
    "            \n",
    "            # Create Path from zero\n",
    "            args_idx = None\n",
    "            for (sta, end), (index, head) in zip(phrase_range, tree):\n",
    "                if sta <= zero_idx < end:\n",
    "                    args_idx = index\n",
    "                    break\n",
    "\n",
    "            predicate_idx = None\n",
    "            path_predicate_before = {}\n",
    "            target = None\n",
    "            for (sta, end), (index, head) in zip(phrase_range[::-1], tree[::-1]):\n",
    "                if target is None and sta <= pas[\"p_id\"] < end:\n",
    "                    path_predicate_before[index] = list(range(sta, end))\n",
    "                    predicate_idx = index\n",
    "                    target = index\n",
    "                if target is not None and target == head:\n",
    "                    path_predicate_before[index] = list(range(sta, end))\n",
    "                    target = index\n",
    "\n",
    "            if predicate_idx < args_idx:\n",
    "                continue\n",
    "                    \n",
    "            merged_indices = []\n",
    "            text_a = []\n",
    "            for k in sorted(path_predicate_before):\n",
    "                if k == predicate_idx:\n",
    "                    for idx in path_predicate_before[k]:\n",
    "                        if idx == pas[\"p_id\"]:\n",
    "                            text_a += [instance[\"bases\"][idx], \"、\"]\n",
    "                            merged_indices.append(idx)\n",
    "                            break\n",
    "                        else:\n",
    "                            text_a += [instance[\"surfaces\"][idx]]\n",
    "                            merged_indices.append(idx)\n",
    "                    break\n",
    "                else:\n",
    "                    merged_indices += path_predicate_before[k]\n",
    "                    text_a += [instance[\"surfaces\"][idx] for idx in path_predicate_before[k]]\n",
    "            n =  random.randint(*n_insert)\n",
    "            length = len(merged_indices)\n",
    "            insert_position = [length]\n",
    "            mask_ids = list(range(length, length + n))\n",
    "            text_a += [MASK] * n\n",
    "            text_a += [instance[\"surfaces\"][zero_idx]]\n",
    "                    \n",
    "            # Create new instance\n",
    "            keys = [\"tokens\", \"surfaces\", \"bases\", \"pos\", \"bunsetsu\", \"tree\"]\n",
    "            new_instance = {k: [instance[k][idx] for idx in merged_indices] for k in keys}\n",
    "            new_instance[\"insert_position\"] = insert_position\n",
    "            new_instance[\"text_a\"] = text_a\n",
    "            new_instance[\"mask_ids\"] = mask_ids\n",
    "            \n",
    "            yield args, new_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "村山富市首相は 年頭に あたり 首相官邸で 内閣記者会と 二十八日 会見し、 社会党の 新民主連合所属議員の 離党問題に ついて 「政権に 影響を 及ぼす ことには ならない。 離党者が いても、 その 範囲に とどまると 思う」と 述べ、 大量離党には 至らないとの 見通しを 示した。\n",
      "問題, が, 及ぼす \n",
      "\t影響を及ぼす \n",
      "\t影響を及ぼす、MMMM問題 \n",
      "\tからMMMM。\n",
      "問題, が, なら \n",
      "\t影響を及ぼすことにはなら \n",
      "\t影響を及ぼすことにはなる、MMM問題 \n",
      "\tをMMMM。\n",
      "問題, が, とどまる \n",
      "\tその範囲にとどまる \n",
      "\tその範囲にとどまる、MMMM問題 \n",
      "\tでMMMM。\n",
      "首相, が, 思う \n",
      "\tその範囲にとどまると思う \n",
      "\tその範囲にとどまると思う、MMMM首相 \n",
      "\tとMMMMM。\n",
      "首相, が, 述べ \n",
      "\tその範囲にとどまると思う」と述べ \n",
      "\tその範囲にとどまると思う」と述べる、MMMMM首相 \n",
      "\tにMMM。\n",
      "問題, が, 至ら \n",
      "\t大量離党には至ら \n",
      "\t大量離党には至る、MMM問題 \n",
      "\tでMMMM。\n",
      "首相, が, 示した \n",
      "\t大量離党には至らないとの見通しを示した \n",
      "\t大量離党には至らないとの見通しを示す、MMM首相 \n",
      "\tにMMMMM。\n",
      "また、 一九九五年中の 衆院解散・総選挙の 可能性に 否定的な 見解を 表明、 二十日 召集予定の 通常国会前の 内閣改造を 明確に 否定した。\n",
      "ロシア南部チェチェン共和国の 首都グロズヌイに 進攻した ロシア軍は 三十一日、 首都中心部を 装甲車などで 攻撃、 大統領官邸など 数カ所が 炎上した。\n",
      "ロシア側は 首都制圧の 最終 段階に 入ったと みられる。\n",
      "制圧, が, 入った \n",
      "\t最終段階に入った \n",
      "\t最終段階に入る、MMMM制圧 \n",
      "\tとMMM。\n",
      "グロズヌイからの 報道では、 ロシア軍は 激しい 空爆と 砲撃を 加えた 後、 装甲車部隊が 大統領官邸付近に 進出。\n",
      "軍, が, 加えた \n",
      "\t空爆と砲撃を加えた \n",
      "\t空爆と砲撃を加える、MMMMM軍 \n",
      "\tよりMMMM。\n",
      "同官邸前などで ドゥダエフ政権部隊と 激しい 市街戦を 展開している。\n",
      "一方、 ドゥダエフ政権側の 首都防衛司令官は 同日 夕、 テレビを 通じ、 首都防衛は うまく いっており、 ロシア軍の 戦車 五十両を 破壊したと 発表。\n",
      "官, が, 通じ \n",
      "\tテレビを通じ \n",
      "\tテレビを通ずる、MMMMM官 \n",
      "\tがMMMM。\n",
      "また、 ドゥダエフ大統領は 現在、 交渉中の ロシア議会の 代表団とともに 防空ごうに 避難しており、 無事と いう。\n",
      "大統領, が, 無事 \n",
      "\t防空ごうに避難しており、無事 \n",
      "\t防空ごうに避難しており、無事だ、MMMMM大統領 \n",
      "\tでMMMM。\n",
      "ドゥダエフ大統領は 三十日 夜、 エリツィン・ロシア大統領に 正月休戦を 提案したが、 ロシア側は これを 黙殺した。\n",
      "村山富市首相の 年頭記者会見の 要旨は 次の 通り。\n"
     ]
    }
   ],
   "source": [
    "PARTICLE = [\"が\", \"の\", \"を\", \"に\", \"へ\", \"と\", \"より\", \"から\", \"で\"]\n",
    "\n",
    "for instance in islice(read_file(file), 10):\n",
    "    for args, instance in create_before_after(instance):\n",
    "        print(\", \".join(args), \n",
    "              \"\\n\\t\"+\"\".join(instance[\"surfaces\"]), \n",
    "              \"\\n\\t\"+\"\".join(instance[\"text_a\"]),\n",
    "              \"\\n\\t\" + random.choice(PARTICLE) + random.randint(3, 5) * MASK + \"。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paths(instance, pas):\n",
    "    \"\"\"\n",
    "    Yields:\n",
    "        Paths： (項の前，項の後ろ，述語の前，述語の後ろ)\n",
    "        triple: (target_word, particle, predicate)\n",
    "        position_type： 'before' or 'after'\n",
    "    \"\"\"\n",
    "    case_converter = {0: \"が\", 1: \"を\", 2: \"に\"}\n",
    "    phrase_ids = [idx for idx, v in enumerate(instance[\"bunsetsu\"]) if v == 1] + [len(instance[\"tokens\"])]\n",
    "    phrase_range = [(sta, end) for sta, end in sliding_window(2, phrase_ids)]\n",
    "    tree = [p for p in instance[\"tree\"] if p]\n",
    "    assert len(phrase_range) == len(tree)\n",
    "\n",
    "    predicate = instance[\"surfaces\"][pas[\"p_id\"]]\n",
    "    zero_ids = [idx for idx, t in enumerate(pas[\"types\"]) if t == \"zero\"]\n",
    "    for zero_idx in zero_ids:\n",
    "        path_zero_before, path_zero_after, path_predicate_before, path_predicate_after,  = {}, {}, {}, {}\n",
    "        predicate_idx, arg_idx = None, None\n",
    "\n",
    "        # Create Path from zero (after)\n",
    "        target = None\n",
    "        for (sta, end), (index, head) in zip(phrase_range, tree):\n",
    "            if target is None and sta <= zero_idx < end:\n",
    "                path_zero_after[index] = list(range(sta, end))\n",
    "                arg_idx = index\n",
    "                target = head\n",
    "            elif target is not None and target == index:\n",
    "                path_zero_after[index] = list(range(sta, end))\n",
    "                target = head\n",
    "\n",
    "        # Create Path from zero (before)\n",
    "        target = None\n",
    "        for (sta, end), (index, head) in zip(phrase_range[::-1], tree[::-1]):\n",
    "            if target is None and sta <= zero_idx < end:\n",
    "                path_zero_before[index] = list(range(sta, end))\n",
    "                target = index\n",
    "            elif target is not None and target == head:\n",
    "                path_zero_before[index] = list(range(sta, end))\n",
    "                target = index\n",
    "\n",
    "        # Create Path from predicate (after)\n",
    "        target = None\n",
    "        for (sta, end), (index, head) in zip(phrase_range, tree):\n",
    "            if target is None and sta <= pas[\"p_id\"] < end:\n",
    "                path_predicate_after[index] = list(range(sta, end))\n",
    "                predicate_idx = index\n",
    "                target = head\n",
    "            if target is not None and target == index:\n",
    "                path_predicate_after[index] = list(range(sta, end))\n",
    "                target = head\n",
    "\n",
    "        # Create Path from predicate (before)\n",
    "        target = None\n",
    "        for (sta, end), (index, head) in zip(phrase_range[::-1], tree[::-1]):\n",
    "            if target is None and sta <= pas[\"p_id\"] < end:\n",
    "                path_predicate_before[index] = list(range(sta, end))\n",
    "                target = index\n",
    "            if target is not None and target == head:\n",
    "                path_predicate_before[index] = list(range(sta, end))\n",
    "                target = index\n",
    "\n",
    "        paths = (path_zero_before, path_zero_after, path_predicate_before, path_predicate_after)\n",
    "        triple = (instance[\"surfaces\"][zero_idx], case_converter[pas[\"args\"][zero_idx]], predicate)\n",
    "        position_type = \"before\" if arg_idx < predicate_idx else \"after\"\n",
    "\n",
    "        yield paths, triple, position_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_b(instance, pas, paths, triple, vocab, n_insert=(3, 5)):\n",
    "    \"\"\"β-b：「項 > 述語」 -> 「項 > 述語」へ 変換\"\"\"\n",
    "    target_word, particle, predicate = triple\n",
    "\n",
    "    # Merge path\n",
    "    path_zero_before, _, path_predicate_before, _ = paths\n",
    "\n",
    "    # Create Instance\n",
    "    before_idx = 0\n",
    "    indices, insert_position = [], {}\n",
    "    tree = [t for t in instance[\"tree\"] if t is not None]\n",
    "    text_a = []\n",
    "\n",
    "    for k in sorted(path_zero_before):\n",
    "        if k <= max(path_predicate_before):\n",
    "            continue\n",
    "        \n",
    "        # Insert MASK\n",
    "        if before_idx + 1 != k:\n",
    "            n = random.randint(*n_insert)\n",
    "            insert_position[len(indices)] = n\n",
    "            text_a += [MASK] * n\n",
    "        indices += path_zero_before[k]\n",
    "        text_a += [instance[\"surfaces\"][idx] for idx in path_zero_before[k]]\n",
    "        before_idx = k\n",
    "\n",
    "    # Insert MASK\n",
    "    n = random.randint(*n_insert)\n",
    "    insert_position[len(indices)] = n\n",
    "    text_a += [MASK] * n + [\"、\", \"、\", target_word, particle]\n",
    "    indices.append(\"、\")\n",
    "\n",
    "    for k in sorted(path_predicate_before):\n",
    "        indices += path_predicate_before[k]\n",
    "        text_a += [instance[\"surfaces\"][idx] for idx in path_predicate_before[k]]\n",
    "    # Insert MASK\n",
    "    if max(path_predicate_before) != tree[-1][0]:\n",
    "        n = random.randint(*n_insert)\n",
    "        insert_position[len(indices)] = n\n",
    "        text_a += [MASK] * n + [\"。\"]\n",
    "        indices.append(\"。\")\n",
    "\n",
    "    tokens = [vocab[i] if type(i) == str else instance[\"tokens\"][i] for i in indices]\n",
    "    p_ids = [1 if idx == pas[\"p_id\"] else 0 for idx in range(len(instance[\"tokens\"]))]\n",
    "    p_id = [0 if type(i) == str else p_ids[i] for i in indices].index(1)\n",
    "    args = [3 if type(i) == str else pas[\"args\"][i] for i in indices]\n",
    "    instance = {\"tokens\": tokens,\n",
    "                \"pas\": {\"p_id\": p_id, \"args\": args},\n",
    "                \"insert_position\": insert_position,\n",
    "                \"text_a\": text_a,\n",
    "                \"black_list\": triple}\n",
    "\n",
    "    return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "CASE_PARTICLES = [\"が\", \"の\", \"を\", \"に\", \"へ\", \"と\", \"より\", \"から\", \"で\"]\n",
    "def alpha_b(instance, pas, paths, triple, vocab, n_insert=(3, 5)):\n",
    "    \"\"\"α-b：「項 < 述語」 -> 「項 > 述語」へ 変換 (が格のみ)\"\"\"\n",
    "    target_word, particle, predicate = triple\n",
    "\n",
    "    # Merge path\n",
    "    _, _, path_predicate_before, _ = paths\n",
    "\n",
    "    # Create Instance\n",
    "    predicate_idx = max(path_predicate_before)\n",
    "    text_a, text_b, indices, insert_position = [], [], [], {}\n",
    "    tree = [t for t in instance[\"tree\"] if t is not None]\n",
    "\n",
    "    for k in sorted(path_predicate_before):\n",
    "        if k == predicate_idx:\n",
    "            for idx in path_predicate_before[k]:\n",
    "                if idx == pas[\"p_id\"]:\n",
    "                    text_a += [instance[\"bases\"][idx], \"、\"]\n",
    "                    indices += [idx, \"、\"]\n",
    "                    break\n",
    "                else:\n",
    "                    text_a += [instance[\"surfaces\"][idx]]\n",
    "                    indices.append(idx)\n",
    "            break\n",
    "        else:\n",
    "            indices += path_predicate_before[k]\n",
    "            text_a += [instance[\"surfaces\"][idx] for idx in path_predicate_before[k]]\n",
    "    # Insert MASK (text_a)\n",
    "    n = random.randint(*n_insert)\n",
    "    insert_position[len(indices)] = n\n",
    "    text_a += [MASK] * n + [target_word]\n",
    "    indices.append(target_word)\n",
    "    # Insert MASK (text_b)\n",
    "    insert_case_particle = random.choice(CASE_PARTICLES)\n",
    "    indices.append(insert_case_particle)\n",
    "\n",
    "    n = random.randint(*n_insert)\n",
    "    insert_position[len(indices)] = n\n",
    "    text_b = [insert_case_particle] + [MASK] * n + [\"。\"]\n",
    "    indices.append(\"。\")\n",
    "\n",
    "    # Create new instance\n",
    "    tokens = [vocab[i] if type(i) == str else instance[\"tokens\"][i] for i in indices]\n",
    "    p_ids = [1 if idx == pas[\"p_id\"] else 0 for idx in range(len(instance[\"tokens\"]))]\n",
    "    p_id = [0 if type(i) == str else p_ids[i] for i in indices].index(1)\n",
    "    args = [3 if type(i) == str else pas[\"args\"][i] for i in indices]\n",
    "    case_converter = {\"が\": 0, \"を\": 1, \"に\": 2}\n",
    "    args[-3] = case_converter[particle]\n",
    "    instance = {\"tokens\": tokens,\n",
    "                \"pas\": {\"p_id\": p_id, \"args\": args},\n",
    "                \"insert_position\": insert_position,\n",
    "                \"text_a\": text_a,\n",
    "                \"text_b\": text_b,\n",
    "                \"black_list\": [target_word, particle]}\n",
    "\n",
    "    return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Load '/Users/ryuto/lab/research/data/raw/NTC_Matsu_original/wordIndex.txt'\n"
     ]
    }
   ],
   "source": [
    "vocab = set_vocab(\"/Users/ryuto/lab/research/data/raw/NTC_Matsu_original/wordIndex.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【beta】\n",
      "首都,に,した\n",
      "MMMM首都中心部をMMMMM、、首都にロシア南部チェチェン共和国の首都グロズヌイに進攻したMMMMM。\n",
      "ロシア南部チェチェン共和国の首都グロズヌイに進攻したロシア軍は三十一日、首都中心部を装甲車などで攻撃、大統領官邸など数カ所が炎上した。\n",
      "【alpha】\n",
      "首都,に\n",
      "ロシア南部チェチェン共和国の首都グロズヌイに進攻する、MMMM首都\n",
      "へMMMMM。\n",
      "ロシア南部チェチェン共和国の首都グロズヌイに進攻したロシア軍は三十一日、首都中心部を装甲車などで攻撃、大統領官邸など数カ所が炎上した。\n",
      "\n",
      "【beta】\n",
      "政党,が,立った\n",
      "MMMMM平和と民主主義を担う政党がMMMM、、政党が市民の側に立ったMMMMM。\n",
      "党内の議論や党関係者の意見は「保守二党論はよろしくない。市民の側に立った平和と民主主義を担う政党が必要」というものだ。\n",
      "【alpha】\n",
      "政党,が\n",
      "市民の側に立つ、MMM政党\n",
      "がMMMM。\n",
      "党内の議論や党関係者の意見は「保守二党論はよろしくない。市民の側に立った平和と民主主義を担う政党が必要」というものだ。\n",
      "\n",
      "【beta】\n",
      "米,が,する\n",
      "MMM今度の日米首脳会談のMMM、、米が二十一世紀を展望するMMM。\n",
      "戦後五十年の節目であり、冷戦構造が崩壊し、世界が新秩序を求めている時期に二十一世紀を展望する日米の親密な関係をどう構築するかが今度の日米首脳会談の大きな課題だ。\n",
      "【alpha】\n",
      "米,が\n",
      "二十一世紀を展望する、MMM米\n",
      "にMMM。\n",
      "戦後五十年の節目であり、冷戦構造が崩壊し、世界が新秩序を求めている時期に二十一世紀を展望する日米の親密な関係をどう構築するかが今度の日米首脳会談の大きな課題だ。\n",
      "\n",
      "【beta】\n",
      "米,が,する\n",
      "MMM今度の日米首脳会談のMMM、、米がどう構築するかがMMMMM。\n",
      "戦後五十年の節目であり、冷戦構造が崩壊し、世界が新秩序を求めている時期に二十一世紀を展望する日米の親密な関係をどう構築するかが今度の日米首脳会談の大きな課題だ。\n",
      "【alpha】\n",
      "米,が\n",
      "どう構築する、MMM米\n",
      "とMMMM。\n",
      "戦後五十年の節目であり、冷戦構造が崩壊し、世界が新秩序を求めている時期に二十一世紀を展望する日米の親密な関係をどう構築するかが今度の日米首脳会談の大きな課題だ。\n",
      "\n",
      "【beta】\n",
      "日本,が,乗り切る\n",
      "MMM「日本はMMMM、、日本が混迷の転換期を乗り切るMMMM。\n",
      "こんな単純な発想にあやうさ、脆さを感じる人は多いでしょうが、混迷の転換期を乗り切るため「日本は変わった」ことの証であり、メッセージになるはずです。\n",
      "【alpha】\n",
      "日本,が\n",
      "混迷の転換期を乗り切る、MMMMM日本\n",
      "をMMMM。\n",
      "こんな単純な発想にあやうさ、脆さを感じる人は多いでしょうが、混迷の転換期を乗り切るため「日本は変わった」ことの証であり、メッセージになるはずです。\n",
      "\n",
      "【beta】\n",
      "もの,が,支え\n",
      "MMMMその後の日本の活力を生んだものはMMMMM、、ものがこれを支え、MMMMM。\n",
      "これを支え、その後の日本の活力を生んだものは占領軍による公職追放という名の“外圧”による世代交代でした。\n",
      "【alpha】\n",
      "もの,が\n",
      "これを支える、MMMMもの\n",
      "をMMMM。\n",
      "これを支え、その後の日本の活力を生んだものは占領軍による公職追放という名の“外圧”による世代交代でした。\n",
      "\n",
      "【beta】\n",
      "時代,が,似た\n",
      "MMMMM必要とする時代にMMMMM、、時代が敗戦直後にも似たMMM。\n",
      "今は敗戦直後にも似た大胆な切開手術を必要とする時代に入っています。\n",
      "【alpha】\n",
      "時代,が\n",
      "敗戦直後にも似る、MMM時代\n",
      "へMMMM。\n",
      "今は敗戦直後にも似た大胆な切開手術を必要とする時代に入っています。\n",
      "\n",
      "【beta】\n",
      "設立,を,した\n",
      "MMM信用組合救済銀行の設立も、MMMM、、設立を「金融秩序維持のため」に発動したMMMM。\n",
      "銀行、証券、生・損保など金融機関は「ある日突然に日本から株式市場が消える悪夢を見る」と訴え、政府・日銀が「金融秩序維持のため」に発動した信用組合救済銀行の設立も、「バブルに踊った“悪徳”金融機関の救済に税金を使うことに閣僚からも批判が出ているが、ヘタをすると大変なことになる。そうなったら国家的犯罪ですよ」とまで危惧するのです。\n",
      "【alpha】\n",
      "設立,を\n",
      "「金融秩序維持のため」に発動する、MMMMM設立\n",
      "とMMMMM。\n",
      "銀行、証券、生・損保など金融機関は「ある日突然に日本から株式市場が消える悪夢を見る」と訴え、政府・日銀が「金融秩序維持のため」に発動した信用組合救済銀行の設立も、「バブルに踊った“悪徳”金融機関の救済に税金を使うことに閣僚からも批判が出ているが、ヘタをすると大変なことになる。そうなったら国家的犯罪ですよ」とまで危惧するのです。\n",
      "\n",
      "【beta】\n",
      "有権者,が,選ぶ\n",
      "MMM側の有権者のMMMMM、、有権者が選ぶMMMM。\n",
      "政治家や選ぶ側の有権者の意識の変革が不可欠ですが、国会議員がやるべき仕事は何なのか。\n",
      "【alpha】\n",
      "有権者,が\n",
      "選ぶ、MMMM有権者\n",
      "がMMMM。\n",
      "政治家や選ぶ側の有権者の意識の変革が不可欠ですが、国会議員がやるべき仕事は何なのか。\n",
      "\n",
      "【beta】\n",
      "村山,を,囲む\n",
      "MMMM学者グループが発表する「村山ビジョン」のMMMMM、、村山を首相を囲むMMMMM。\n",
      "今月中に首相を囲む学者グループが発表する「村山ビジョン」の基本的な考えを示したもの。\n",
      "【alpha】\n",
      "村山,を\n",
      "首相を囲む、MMMMM村山\n",
      "からMMM。\n",
      "今月中に首相を囲む学者グループが発表する「村山ビジョン」の基本的な考えを示したもの。\n",
      "\n",
      "【beta】\n",
      "分野,を,銘打った\n",
      "MMMM非軍事分野のMMM、、分野を世界平和の創造」と銘打ったMMMMM。\n",
      "「わが国にふさわしい国際貢献による世界平和の創造」と銘打った非軍事分野の国際貢献など「四つの創造」を打ち出している。\n",
      "【alpha】\n",
      "分野,を\n",
      "世界平和の創造」と銘打つ、MMMM分野\n",
      "とMMM。\n",
      "「わが国にふさわしい国際貢献による世界平和の創造」と銘打った非軍事分野の国際貢献など「四つの創造」を打ち出している。\n",
      "\n",
      "【beta】\n",
      "課題,が,懸案\n",
      "MMMM困難な課題にMMM、、課題が長い懸案だったMMM。\n",
      "村山内閣となり、長い懸案だった政治改革、税制改革、被爆者援護法など困難な課題に大きな区切りをつけることができた。\n",
      "【alpha】\n",
      "課題,が\n",
      "長い懸案、MMMMM課題\n",
      "へMMMMM。\n",
      "村山内閣となり、長い懸案だった政治改革、税制改革、被爆者援護法など困難な課題に大きな区切りをつけることができた。\n",
      "\n",
      "【beta】\n",
      "社会,が,自由で\n",
      "MMMM活力ある経済社会のMMMMM、、社会が「自由でMMMMM。\n",
      "第一は「自由で活力ある経済社会の創造」。\n",
      "【alpha】\n",
      "社会,が\n",
      "「自由だ、MMM社会\n",
      "よりMMMMM。\n",
      "第一は「自由で活力ある経済社会の創造」。\n",
      "\n",
      "【beta】\n",
      "さきがけ,に,し\n",
      "MMMさきがけ内部にはMMM、、さきがけにさきがけに期待し、MMMMM。\n",
      "久保氏はさきがけに期待し、武村正義代表と頻繁に接触、武村氏は連携に傾いていると言われるが、さきがけ内部には「社民主義と保守リベラルは違う」などの慎重論は強いという。\n",
      "【alpha】\n",
      "さきがけ,に\n",
      "さきがけに期待する、MMMMMさきがけ\n",
      "にMMMMM。\n",
      "久保氏はさきがけに期待し、武村正義代表と頻繁に接触、武村氏は連携に傾いていると言われるが、さきがけ内部には「社民主義と保守リベラルは違う」などの慎重論は強いという。\n",
      "\n",
      "【beta】\n",
      "会,が,支え\n",
      "MMM社民リベラル政治をすすめる会」はMMMM、、会が「村山政権を支えMMMMM。\n",
      "新党慎重派の「村山政権を支え社民リベラル政治をすすめる会」は「参院選後で十分」との姿勢で、妥協点を探るのは難しそうだ。\n",
      "【alpha】\n",
      "会,が\n",
      "「村山政権を支える、MMMMM会\n",
      "とMMM。\n",
      "新党慎重派の「村山政権を支え社民リベラル政治をすすめる会」は「参院選後で十分」との姿勢で、妥協点を探るのは難しそうだ。\n",
      "\n",
      "【beta】\n",
      "離党,を,したら\n",
      "MMMM離党にはMMM、、離党を「地元の支持労組に相談したら、MMM。\n",
      "新民連幹部の一人は「地元の支持労組に相談したら、離党には難色を示された」と証言する。\n",
      "【alpha】\n",
      "離党,を\n",
      "「地元の支持労組に相談する、MMMMM離党\n",
      "へMMMMM。\n",
      "新民連幹部の一人は「地元の支持労組に相談したら、離党には難色を示された」と証言する。\n",
      "\n",
      "【beta】\n",
      "ＡＰＥＣ,が,開か\n",
      "MMMＡＰＥＣのMMMMM、、ＡＰＥＣがインドネシアで開かれたMMMM。\n",
      "情報サミットは、九四年十一月にインドネシアで開かれたＡＰＥＣの非公式首脳会議で、韓国の金泳三大統領が内々に提唱したものの非公開扱いとなり、会議終了後に発表されたボゴール宣言にも盛り込まれなかった。\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'ＡＰＥＣ'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-2f6d76d63c03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mnew_instance2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"【alpha】\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_instance2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"black_list\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-125-e923b5cd2dc4>\u001b[0m in \u001b[0;36malpha_b\u001b[0;34m(instance, pas, paths, triple, vocab, n_insert)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Create new instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mp_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"p_id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mp_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mp_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-125-e923b5cd2dc4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Create new instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mp_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"p_id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mp_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mp_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ＡＰＥＣ'"
     ]
    }
   ],
   "source": [
    "for instance in islice(read_file(file), 200):\n",
    "    for pas in instance[\"pas\"]:\n",
    "        for paths, triple, position_type in extract_paths(instance, pas):\n",
    "            if position_type == \"after\":\n",
    "                new_instance = beta_b(instance, pas, paths, triple, vocab)\n",
    "                print(\"【beta】\")\n",
    "                print(\",\".join(new_instance[\"black_list\"]))\n",
    "                print(\"\".join(new_instance[\"text_a\"]))\n",
    "                print(\"\".join(instance[\"surfaces\"]))\n",
    "\n",
    "                \n",
    "                new_instance2 = alpha_b(instance, pas, paths, triple, vocab)\n",
    "                print(\"【alpha】\")\n",
    "                print(\",\".join(new_instance2[\"black_list\"]))\n",
    "                print(\"\".join(new_instance2[\"text_a\"]))\n",
    "                print(\"\".join(new_instance2[\"text_b\"]))\n",
    "                print(\"\".join(instance[\"surfaces\"]))\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
